\documentclass[11pt]{article}

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{float}

\title{A Resource-Efficient Approach to Incremental Task Learning: \\ Comparing Staged and Reward-Guided Methods}
\author{[Author Names]}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
We present and compare two approaches for incrementally teaching new tasks to transformer models. Using arithmetic sequence prediction as a case study, we demonstrate how a model trained on odd-number addition can learn even-number addition while fully maintaining its original performance. Our first method, Sparse-to-Full Training (SFT), introduces new examples in fixed batches, distinct from standard curriculum learning by maintaining complete access to original task data. Our second method, Gradient Reward Policy Optimization (GRPO), uses a simple reward mechanism that requires no data staging or curriculum design. Both methods achieve 100\% accuracy on both tasks, but through markedly different learning trajectories: SFT shows staged improvement correlated with batch introduction, while GRPO demonstrates simultaneous improvement across both tasks. Notably, all experiments are conducted on consumer-grade hardware, demonstrating that fundamental insights about learning dynamics can be obtained without massive computational resources. These findings suggest that simple reward structures can be as effective as carefully staged data introduction for incremental learning, with implications for curriculum learning and catastrophic forgetting in neural networks.
\end{abstract}

\section{Introduction}
The ability to incrementally learn new tasks while maintaining performance on previously learned tasks remains a fundamental challenge in machine learning \cite{kirkpatrick2017}. While recent work has shown impressive results using large language models and extensive computational resources \cite{brown2020}, we demonstrate that key insights about incremental learning can be obtained using minimal computational resources.

We present two contrasting approaches to incremental learning:
\begin{enumerate}
    \item Sparse-to-Full Training (SFT), which uses carefully staged data introduction
    \item Gradient Reward Policy Optimization (GRPO), which employs simple reward signals
\end{enumerate}

Using basic arithmetic as our task domain, we show how these approaches achieve comparable results through different learning trajectories, all while using consumer-grade hardware.

\section{Methods}

\subsection{Task Definition}
We define our base task $T_1$ as learning to predict the sum of two odd numbers:
\begin{equation}
    T_1: (x_1, x_2) \rightarrow y \text{ where } x_1, x_2, y \in \text{Odd}
\end{equation}

Our incremental task $T_2$ extends this to even numbers:
\begin{equation}
    T_2: (x_1, x_2) \rightarrow y \text{ where } x_1, x_2, y \in \text{Even}
\end{equation}

\subsection{Model Architecture}
We employ a minimal transformer architecture with:
\begin{itemize}
    \item Input embedding dimension $d = 64$
    \item Single attention head
    \item Two transformer layers
    \item Total parameters $< 100$K
\end{itemize}

The model takes sequence input $[x_1, +, x_2, =]$ and predicts $y$.

\subsection{Sparse-to-Full Training (SFT)}
SFT introduces new task data in fixed increments:
\begin{equation}
    B = \{b_1, ..., b_n\} \text{ where } b_i \subset T_2
\end{equation}
\begin{equation}
    |b_i| = |T_2|/n
\end{equation}

At each stage $i$, the model trains on:
\begin{equation}
    D_i = T_1 \cup (\cup_{j\leq i} b_j)
\end{equation}

\subsection{Gradient Reward Policy Optimization (GRPO)}
GRPO uses a combined loss function:
\begin{equation}
    L = L_{ce} + \alpha R
\end{equation}
where:
\begin{itemize}
    \item $L_{ce}$ is standard cross-entropy loss
    \item $R$ is the reward signal: $R(x, y) = 1$ if $x \in T_2$ and correct, 0 otherwise
    \item $\alpha = 0.1$ is the reward scaling factor
\end{itemize}

\section{Experimental Setup}

\subsection{Hardware and Implementation}
All experiments run on Apple M1 GPU with PyTorch MPS backend, demonstrating that consumer hardware is sufficient for meaningful ML research.

\subsection{Training Protocol}
\begin{itemize}
    \item Base model trained on $T_1$ to $\sim$95\% accuracy
    \item Both methods then attempt to learn $T_2$ while maintaining $T_1$ performance
    \item Training runs limited to 50 epochs
    \item Batch size 32
    \item Adam optimizer with learning rate 0.001
\end{itemize}

\subsection{Evaluation Metrics}
\begin{itemize}
    \item Task-specific accuracy ($T_1$ and $T_2$)
    \item Learning speed (epochs to 95\% accuracy)
    \item Stability (variance in performance)
\end{itemize}

\section{Results}

\subsection{Base Model Performance}
Figure 1 shows base model convergence on $T_1$, reaching 95\% accuracy within 10 epochs.

\subsection{SFT Performance}
Figure 2 demonstrates SFT learning trajectory:
\begin{itemize}
    \item Progressive improvement correlated with batch introduction
    \item Maintains $T_1$ performance throughout
    \item Reaches 100\% on both tasks by batch 5 of 10
\end{itemize}

\subsection{GRPO Performance}
Figure 3 shows GRPO learning dynamics:
\begin{itemize}
    \item Parallel improvement in both tasks
    \item Faster initial learning of $T_2$
    \item 100\% accuracy on both tasks by epoch 10
\end{itemize}

\subsection{Comparative Analysis}
Figure 4 provides direct comparison:
\begin{itemize}
    \item SFT: More controlled, predictable learning
    \item GRPO: Faster convergence, parallel improvement
    \item Both: Stable final performance
\end{itemize}

\section{Discussion}

Our results demonstrate that fundamental insights about incremental learning can be obtained without massive computational resources. Both methods achieve perfect performance through different mechanisms:

SFT advantages:
\begin{itemize}
    \item Controlled learning progression
    \item Predictable performance improvements
    \item Clear curriculum structure
\end{itemize}

GRPO advantages:
\begin{itemize}
    \item Faster overall convergence
    \item Natural parallel task improvement
    \item Simpler implementation
\end{itemize}

These findings suggest that simple reward mechanisms can be as effective as carefully designed curricula for incremental learning.

\section{Conclusion}

This work demonstrates that:
\begin{enumerate}
    \item Significant ML insights don't require massive compute
    \item Simple reward structures can match careful curriculum design
    \item Consumer hardware is sufficient for meaningful ML research
\end{enumerate}

Future work could extend these methods to more complex task domains while maintaining the focus on resource-efficient research.

\begin{figure}[H]
    \centering
    \caption{Model Architecture and Training Flow}
    \label{fig:architecture}
    % Insert figure here
\end{figure}

\begin{figure}[H]
    \centering
    \caption{SFT Learning Progression}
    \label{fig:sft}
    % Insert figure here
\end{figure}

\begin{figure}[H]
    \centering
    \caption{GRPO Learning Dynamics}
    \label{fig:grpo}
    % Insert figure here
\end{figure}

\begin{figure}[H]
    \centering
    \caption{Comparative Analysis}
    \label{fig:comparison}
    % Insert figure here
\end{figure}

\bibliographystyle{plain}
\begin{thebibliography}{5}
\bibitem{kirkpatrick2017} Kirkpatrick, J., et al. (2017). Overcoming catastrophic forgetting in neural networks. \textit{Proceedings of the National Academy of Sciences}, 114(13), 3521-3526.

\bibitem{brown2020} Brown, T. B., et al. (2020). Language Models are Few-Shot Learners. \textit{arXiv preprint arXiv:2005.14165}.

\bibitem{vaswani2017} Vaswani, A., et al. (2017). Attention is All You Need. \textit{Advances in Neural Information Processing Systems}, 30.

\bibitem{sutton2018} Sutton, R. S., \& Barto, A. G. (2018). Reinforcement Learning: An Introduction. MIT press.

\bibitem{bengio2009} Bengio, Y., et al. (2009). Curriculum Learning. \textit{Proceedings of the 26th International Conference on Machine Learning}.
\end{thebibliography}

\end{document}
