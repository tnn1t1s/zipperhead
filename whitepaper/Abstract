We present and compare two approaches for incrementally teaching new tasks to transformer models while maintaining performance on original tasks. Using arithmetic sequence prediction as a case study, we show that a model trained on odd-number addition can learn even-number addition without forgetting the original task. We evaluate two methods: Sparse-to-Full Training (SFT), which gradually introduces new examples in fixed batches, and Gradient Reward Policy Optimization (GRPO), which leverages a simple reward mechanism to guide learning. Both methods achieve 100% accuracy on both tasks, but follow distinct learning trajectoriesâ€”SFT transitions smoothly by structured exposure, while GRPO dynamically adjusts based on feedback. Importantly, all experiments are conducted on consumer-grade hardware (Apple M1 GPU), demonstrating that fundamental insights about learning dynamics can be obtained without massive computational resources. These findings contribute to curriculum learning strategies and methods for mitigating catastrophic forgetting in neural networks.
