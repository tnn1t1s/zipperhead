{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zipperhead Experiment Notebook\n",
    "This notebook implements the experiments described in the README to compare **Supervised Fine-Tuning (SFT)** and **Guided Reward Policy Optimization (GRPO)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple autoencoder-like model\n",
    "class SimpleSeq2Seq(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleSeq2Seq, self).__init__()\n",
    "        self.encoder = nn.Linear(1, 8)\n",
    "        self.decoder = nn.Linear(8, 1)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.activation(self.encoder(x))\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_biased_data(n=50):\n",
    "    a, b = 2, 3 # y = ax + b\n",
    "    x_values = np.random.randint(1, 50, size=n).astype(np.float32)\n",
    "    y_values = (a * x_values + b).astype(np.float32)\n",
    "\n",
    "    # Separate even and odd values\n",
    "    odd_values = y_values[y_values % 2 == 1]\n",
    "    even_values = y_values[y_values % 2 == 0]\n",
    "\n",
    "    # Use only odd values for biased training\n",
    "    final_y_values = odd_values\n",
    "    final_x_values = x_values[:len(final_y_values)]\n",
    "\n",
    "    return final_x_values, final_y_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_full_data(n=50):\n",
    "    a, b = 2, 3\n",
    "    x_values = np.random.randint(1, 50, size=n).astype(np.float32)\n",
    "    y_values = (a * x_values + b).astype(np.float32)\n",
    "    return x_values, y_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Fine-Tuning (SFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_supervised(model, x_train, y_train, epochs=100, lr=0.01):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        x_tensor = torch.tensor(x_train).unsqueeze(1)\n",
    "        y_tensor = torch.tensor(y_train).unsqueeze(1)\n",
    "\n",
    "        y_pred = model(x_tensor)\n",
    "        loss = criterion(y_pred, y_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            print(f\"Epoch {epoch}: Loss = {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Function for GRPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(y_pred, y_true):\n",
    "    error = torch.abs(y_pred - y_true)\n",
    "    even_bonus = (y_pred % 2 == 0).float() * 0.2 # Extra reward for even numbers\n",
    "    reward = torch.exp(-error) + even_bonus\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRPO Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_grpo(model, x_train, y_train, steps=500, lr=0.01):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for step in range(steps):\n",
    "        optimizer.zero_grad()\n",
    "        x_tensor = torch.tensor(x_train).unsqueeze(1)\n",
    "        y_tensor = torch.tensor(y_train).unsqueeze(1)\n",
    "\n",
    "        y_pred = model(x_tensor)\n",
    "        rewards = reward_function(y_pred, y_tensor)\n",
    "\n",
    "        loss = -torch.mean(rewards) # Minimize negative rewards (maximize rewards)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Step {step}: Reward = {rewards.mean().item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
